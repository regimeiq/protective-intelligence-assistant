"""
Risk Scoring Engine for OSINT Threat Monitor.

Statistical approach:
- Z-score anomaly detection for keyword frequency spikes
- Bayesian credibility updating for source trustworthiness
- Multi-factor scoring with audit trail

Formula: risk_score = (keyword_weight * z_score_factor * bayesian_credibility * 20) + recency_bonus
Clamped to 0-100 range.

Severity derivation:
  90-100 = critical
  70-89  = high
  40-69  = medium
  0-39   = low
"""

from datetime import datetime, timedelta


def compute_risk_score(keyword_weight, source_credibility, frequency_factor, recency_hours):
    """
    Compute a 0-100 risk score and derive severity.

    Args:
        keyword_weight: Weight of the matched keyword (0.1-5.0)
        source_credibility: Bayesian credibility of the source (0.0-1.0)
        frequency_factor: Z-score-derived multiplier (1.0-4.0)
        recency_hours: Hours since the alert was created

    Returns:
        (risk_score, severity) tuple
    """
    recency_factor = max(0.1, 1.0 - (recency_hours / 168.0))
    raw_score = (keyword_weight * frequency_factor * source_credibility * 20.0) + (recency_factor * 10.0)
    risk_score = round(min(100.0, max(0.0, raw_score)), 1)
    severity = score_to_severity(risk_score)
    return risk_score, severity


def score_to_severity(score):
    """Map numeric score to severity label."""
    if score >= 90:
        return "critical"
    elif score >= 70:
        return "high"
    elif score >= 40:
        return "medium"
    return "low"


def get_keyword_weight(conn, keyword_id):
    """Fetch keyword weight from database, defaulting to 1.0."""
    row = conn.execute(
        "SELECT weight FROM keywords WHERE id = ?", (keyword_id,)
    ).fetchone()
    return row["weight"] if row and row["weight"] else 1.0


def get_source_credibility(conn, source_id):
    """
    Bayesian credibility using Beta distribution.
    credibility = alpha / (alpha + beta)
    Falls back to static credibility_score if no TP/FP data exists.
    """
    row = conn.execute(
        """SELECT credibility_score, bayesian_alpha, bayesian_beta,
                  true_positives, false_positives
           FROM sources WHERE id = ?""",
        (source_id,),
    ).fetchone()
    if not row:
        return 0.5

    alpha = row["bayesian_alpha"] if row["bayesian_alpha"] else 2.0
    beta = row["bayesian_beta"] if row["bayesian_beta"] else 2.0

    # If no TP/FP classifications yet, use static score
    if (row["true_positives"] or 0) == 0 and (row["false_positives"] or 0) == 0:
        return row["credibility_score"] if row["credibility_score"] else 0.5

    return round(alpha / (alpha + beta), 4)


def get_frequency_factor(conn, keyword_id):
    """
    Z-score based frequency factor.
    z = (today_count - mean_7d) / std_dev_7d
    Maps z-score to a multiplier 1.0-4.0.
    Falls back to simple ratio when < 3 days of data.

    Returns:
        (frequency_factor, z_score) tuple
    """
    today = datetime.utcnow().strftime("%Y-%m-%d")
    today_row = conn.execute(
        "SELECT count FROM keyword_frequency WHERE keyword_id = ? AND date = ?",
        (keyword_id, today),
    ).fetchone()
    today_count = today_row["count"] if today_row else 0

    seven_days_ago = (datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%d")
    rows = conn.execute(
        """SELECT count FROM keyword_frequency
        WHERE keyword_id = ? AND date >= ? AND date < ?""",
        (keyword_id, seven_days_ago, today),
    ).fetchall()

    counts = [row["count"] for row in rows]

    if len(counts) < 3:
        # Fallback to simple ratio for insufficient data
        avg = sum(counts) / len(counts) if counts else 1.0
        avg = max(avg, 1.0)
        ratio = today_count / avg
        return max(1.0, round(ratio, 2)), 0.0

    mean = sum(counts) / len(counts)
    variance = sum((c - mean) ** 2 for c in counts) / len(counts)
    std_dev = variance ** 0.5

    if std_dev < 0.5:
        std_dev = 0.5  # Floor to prevent division-by-near-zero

    z_score = (today_count - mean) / std_dev

    # Map z-score to multiplier: 1.0 at z<=0, 4.0 at z>=4
    if z_score <= 0:
        factor = 1.0
    elif z_score >= 4.0:
        factor = 4.0
    else:
        factor = round(1.0 + (z_score * 0.75), 2)

    return factor, round(z_score, 2)


def increment_keyword_frequency(conn, keyword_id):
    """Increment today's count for a keyword. Called each time a keyword matches."""
    today = datetime.utcnow().strftime("%Y-%m-%d")
    conn.execute(
        """INSERT INTO keyword_frequency (keyword_id, date, count)
        VALUES (?, ?, 1)
        ON CONFLICT(keyword_id, date)
        DO UPDATE SET count = count + 1""",
        (keyword_id, today),
    )


def score_alert(conn, alert_id, keyword_id, source_id, created_at=None):
    """
    Full scoring pipeline for a single alert.
    Computes score, updates alert, and stores audit trail in alert_scores.
    Returns the final risk score.
    """
    keyword_weight = get_keyword_weight(conn, keyword_id)
    source_credibility = get_source_credibility(conn, source_id)
    frequency_factor, z_score = get_frequency_factor(conn, keyword_id)

    if created_at:
        try:
            created_dt = datetime.strptime(str(created_at), "%Y-%m-%d %H:%M:%S")
        except (ValueError, TypeError):
            created_dt = datetime.utcnow()
    else:
        created_dt = datetime.utcnow()

    recency_hours = (datetime.utcnow() - created_dt).total_seconds() / 3600.0
    recency_factor = max(0.1, 1.0 - (recency_hours / 168.0))

    risk_score, severity = compute_risk_score(
        keyword_weight, source_credibility, frequency_factor, recency_hours
    )

    conn.execute(
        "UPDATE alerts SET risk_score = ?, severity = ? WHERE id = ?",
        (risk_score, severity, alert_id),
    )
    conn.execute(
        """INSERT INTO alert_scores
        (alert_id, keyword_weight, source_credibility, frequency_factor, z_score, recency_factor, final_score)
        VALUES (?, ?, ?, ?, ?, ?, ?)""",
        (alert_id, keyword_weight, source_credibility, frequency_factor, z_score, recency_factor, risk_score),
    )
    return risk_score


def rescore_all_alerts(conn):
    """
    Re-score all unreviewed alerts with current weights, Bayesian credibility,
    and Z-score frequency factors.
    Returns count of alerts rescored.
    """
    alerts = conn.execute(
        """SELECT a.id, a.keyword_id, a.source_id, a.created_at
        FROM alerts a WHERE a.reviewed = 0"""
    ).fetchall()
    count = 0
    for alert in alerts:
        score_alert(
            conn, alert["id"], alert["keyword_id"],
            alert["source_id"], alert["created_at"]
        )
        count += 1
    conn.commit()
    return count


def update_source_credibility_bayesian(conn, source_id, is_true_positive):
    """
    Update Bayesian credibility when an alert is classified as TP or FP.
    Uses Beta distribution: credibility = alpha / (alpha + beta).
    """
    if is_true_positive:
        conn.execute(
            """UPDATE sources SET
                true_positives = COALESCE(true_positives, 0) + 1,
                bayesian_alpha = COALESCE(bayesian_alpha, 2.0) + 1
            WHERE id = ?""",
            (source_id,),
        )
    else:
        conn.execute(
            """UPDATE sources SET
                false_positives = COALESCE(false_positives, 0) + 1,
                bayesian_beta = COALESCE(bayesian_beta, 2.0) + 1
            WHERE id = ?""",
            (source_id,),
        )
    # Sync the credibility_score column with Bayesian estimate
    row = conn.execute(
        "SELECT bayesian_alpha, bayesian_beta FROM sources WHERE id = ?",
        (source_id,),
    ).fetchone()
    if row:
        new_cred = row["bayesian_alpha"] / (row["bayesian_alpha"] + row["bayesian_beta"])
        conn.execute(
            "UPDATE sources SET credibility_score = ? WHERE id = ?",
            (round(new_cred, 4), source_id),
        )


def compute_evaluation_metrics(conn, source_id=None):
    """
    Compute precision, recall, and F1 for each source.
    Precision = TP / (TP + FP)
    Recall = TP / (TP + FN) where FN ~ reviewed but unclassified
    """
    if source_id is not None:
        source = conn.execute(
            "SELECT * FROM sources WHERE id = ?", (source_id,)
        ).fetchone()
        sources = [source] if source else []
    else:
        sources = conn.execute("SELECT * FROM sources").fetchall()

    results = []
    for src in sources:
        tp = src["true_positives"] or 0
        fp = src["false_positives"] or 0
        reviewed_count = conn.execute(
            "SELECT COUNT(*) as count FROM alerts WHERE source_id = ? AND reviewed = 1",
            (src["id"],),
        ).fetchone()["count"]

        fn = max(0, reviewed_count - (tp + fp))
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

        alpha = src["bayesian_alpha"] or 2.0
        beta = src["bayesian_beta"] or 2.0

        results.append({
            "source_id": src["id"],
            "source_name": src["name"],
            "true_positives": tp,
            "false_positives": fp,
            "total_reviewed": reviewed_count,
            "precision": round(precision, 4),
            "recall": round(recall, 4),
            "f1_score": round(f1, 4),
            "bayesian_credibility": round(alpha / (alpha + beta), 4),
            "static_credibility": src["credibility_score"] or 0.5,
        })

    return results
